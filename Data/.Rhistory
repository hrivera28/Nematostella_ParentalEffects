plotQualityProfile(fnRs[1:10])
#Truncates samples based on when the quality starts to drop off
# let's keep the maxEE at 2, we had it at 4 previously but I don't think it's necessary now.
out5 <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(185,185),
maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=TRUE, matchIDs = TRUE)
out5
out4
#constructs a phyloseq object from the dada2 outputs
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa))
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps_otu_table<-as.data.frame(ps@otu_table)
ps_otu_table<-as.data.frame(ps@otu_table)
ps_taxa_table<-as.data.frame(ps@tax_table)
ps_taxa_table<-as.data.frame(ps@tax_table)
ps_df <- as.data.frame(sample_data(ps)) # Put sample_data into a ggplot-friendly data.frame
ps_df$LibrarySize <- sample_sums(ps)
ps_df <- ps_df[order(ps_df$LibrarySize),]
ps_df$Index <- seq(nrow(ps_df))
ggplot(data=ps_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
removal<- subset_taxa(ps, Kingdom !="Eukaryota")
# From 847 to 799
removal2 <- subset_taxa(removal, Order !="Chloroplast")
# From 799 to 642 #So now there were more assigned to choloroplast
removal3 <- subset_taxa(removal2, Family !="Mitochondria")
bact_df <- as.data.frame(sample_data(removal3))
bact_df$LibrarySize <- sample_sums(removal3)
bact_df <- bact_df[order(bact_df$LibrarySize),]
bact_df$Index <- seq(nrow(bact_df))
ggplot(data=bact_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
# Plot abundance for each ASV by samples
long<-ps_otu_table
long$sample<-rownames(ps_otu_table)
long<-gather(long, "ASV","Counts", 1:847)
View(long)
View(ps_otu_table)
View(ps_taxa_table)
ps@refseq[2]
dna
dan[2]
dna[2]
View(dna)
dna@ranges[2]
dna@ranges[1]
#inspects distribution of sequence lengths
table(nchar(getSequences(seqtab)))
View(ps_taxa_table)
View(ps_otu_table)
#Truncates samples based on when the quality starts to drop off
# let's keep the maxEE at 2, we had it at 4 previously but I don't think it's necessary now.
out5 <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(200,200),
maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=TRUE, matchIDs = TRUE)
# On Windows set multithread=FALSE
out
out2 #145 with matchIDs
out3 #155
out4 #165
out5 #185
out5 #185
out5 #185
#creates error charts for the trimmed data
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
#applies core sample inference algorithm to infer true sequence variants from
#filtered and trimmed data
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
#inspects the returned dadaFs object
dadaFs[[1]]
#merges the forward and reverse reads and displays them
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]])
#constructs an ASV table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
#inspects distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#removees chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
#checks the number of reads that make it through each step of the pipeline to track data
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
track
track
taxa <- assignTaxonomy(seqtab.nochim, "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/silva_nr_v138_train_set.fa.gz" , multithread=TRUE, tryRC=TRUE)
samples.out <- rownames(seqtab.nochim)
rownames(samdf) <- samples.out
#constructs a phyloseq object from the dada2 outputs
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa))
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps_otu_table<-as.data.frame(ps@otu_table)
ps_taxa_table<-as.data.frame(ps@tax_table)
ps_df <- as.data.frame(sample_data(ps)) # Put sample_data into a ggplot-friendly data.frame
ps_df$LibrarySize <- sample_sums(ps)
ps_df <- ps_df[order(ps_df$LibrarySize),]
ps_df$Index <- seq(nrow(ps_df))
ggplot(data=ps_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
View(ps_taxa_table)
View(ps_otu_table)
removal<- subset_taxa(ps, Kingdom !="Eukaryota")
# From 847 to 799
removal2 <- subset_taxa(removal, Order !="Chloroplast")
# From 799 to 642 #So now there were more assigned to choloroplast
removal3 <- subset_taxa(removal2, Family !="Mitochondria")
# From 710 to 568, so there were way more that were assigned to Mito now too.
bact_df <- as.data.frame(sample_data(removal3))
bact_df$LibrarySize <- sample_sums(removal3)
bact_df <- bact_df[order(bact_df$LibrarySize),]
bact_df$Index <- seq(nrow(bact_df))
ggplot(data=bact_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
# Plot abundance for each ASV by samples
long<-ps_otu_table
long$sample<-rownames(ps_otu_table)
long<-gather(long, "ASV","Counts", 1:847)
ps.trans <- transform_sample_counts(removal3, function(OTU) OTU/sum(OTU))
View(ps.trans)
cleanotus <- psmelt(ps.trans)
ggplot(cleanotus, aes(x=Sample, y=Abundance))+geom_bar(stat="identity")
#percent_abundance based on symstate
graph1 <- ddply(subset(cleanotus,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
graph1
ggplot(graph1, aes(x=treatment, y=percent_abundance, fill=Family))+geom_bar(stat="identity")
ggplot(graph1, aes(x=treatment, y=percent_abundance, fill=Family))+geom_bar(stat="identity")
getwd()
ggsave("test_plot.png", width=30, height=20, units="in")
#creates error charts for the trimmed data
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
#applies core sample inference algorithm to infer true sequence variants from
#filtered and trimmed data
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
#inspects the returned dadaFs object
dadaFs[[1]]
#merges the forward and reverse reads and displays them
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]])
#constructs an ASV table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
#inspects distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#removees chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
#checks the number of reads that make it through each step of the pipeline to track data
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
track
track
taxa <- assignTaxonomy(seqtab.nochim, "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/silva_nr_v138_train_set.fa.gz" , multithread=TRUE, tryRC=TRUE)
samples.out <- rownames(seqtab.nochim)
rownames(samdf) <- samples.out
#constructs a phyloseq object from the dada2 outputs
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa))
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps_otu_table<-as.data.frame(ps@otu_table)
ps_taxa_table<-as.data.frame(ps@tax_table)
ps_df <- as.data.frame(sample_data(ps)) # Put sample_data into a ggplot-friendly data.frame
ps_df$LibrarySize <- sample_sums(ps)
ps_df <- ps_df[order(ps_df$LibrarySize),]
ps_df$Index <- seq(nrow(ps_df))
ggplot(data=ps_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
View(ps_otu_table)
View(ps_taxa_table)
removal<- subset_taxa(ps, Kingdom !="Eukaryota")
# From 847 to 799
removal2 <- subset_taxa(removal, Order !="Chloroplast")
# From 799 to 642 #So now there were more assigned to choloroplast
removal3 <- subset_taxa(removal2, Family !="Mitochondria")
# From 710 to 568, so there were way more that were assigned to Mito now too.
bact_df <- as.data.frame(sample_data(removal3))
bact_df$LibrarySize <- sample_sums(removal3)
bact_df <- bact_df[order(bact_df$LibrarySize),]
bact_df$Index <- seq(nrow(bact_df))
ggplot(data=bact_df, aes(x=Index, y=LibrarySize, color=treatment)) + geom_point()
ps.trans <- transform_sample_counts(removal3, function(OTU) OTU/sum(OTU))
cleanotus <- psmelt(ps.trans)
ggplot(cleanotus, aes(x=Sample, y=Abundance))+geom_bar(stat="identity")
#percent_abundance based on symstate
graph1 <- ddply(subset(cleanotus,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ggplot(graph1, aes(x=treatment, y=percent_abundance, fill=Family))+geom_bar(stat="identity")
ggplot(graph1, aes(x=treatment, y=percent_abundance, fill=Family))+geom_bar(stat="identity")+theme(legend.position = "none")
#sets the path
path <- "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Oculina_ITS/input_files"
list.files(path)
fnFs2 <- sort(list.files(path, pattern="_R1_fordada.fastq.gz", full.names = TRUE))
fnRs2 <- sort(list.files(path, pattern="_R2_fordada.fastq.gz", full.names = TRUE))
#extracts sample names and displays them
sample.names <- sapply(strsplit(basename(fnFs2), "_R"), '[',1)
head(sample.names)
#creates quality profiles to assist in trimming
plotQualityProfile(fnFs2[1:10])
plotQualityProfile(fnRs2[1:10])
#creates a new subdirectory called "filtered"
filtFs2 <- file.path("/Users/jackweldon/Desktop/Code", "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs2 <- file.path("/Users/jackweldon/Desktop/Code", "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
#creates a new subdirectory called "filtered"
filtFs2 <- file.path("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Oculina_ITS/input_files/", "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs2 <- file.path("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Oculina_ITS/input_files/", "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs2) <- sample.names
names(filtRs2) <- sample.names
#truncates samples at 140 bp
out_its <- filterAndTrim(fnFs2, filtFs2, fnRs2, filtRs2, truncLen=c(225,225),
maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=TRUE, matchIDs = TRUE) # On Windows set multithread=FALSE
#creates a new subdirectory called "filtered"
filtFs2 <- file.path("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Oculina_ITS/input_files", "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs2 <- file.path("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Oculina_ITS/input_files", "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
#creates error charts for the trimmed data
errF2 <- learnErrors(filtFs2, multithread=TRUE)
errR2 <- learnErrors(filtRs2, multithread=TRUE)
plotErrors(errF2, nominalQ=TRUE)
plotErrors(errR2, nominalQ=TRUE)
#applies core sample inference algorithm to infer true sequence variants from
#filtered and trimmed data
dadaFs2 <- dada(filtFs2, err=errF2, multithread=TRUE)
dadaRs2 <- dada(filtFs2, err=errR2, multithread=TRUE)
dadaRs2 <- dada(filtRs2, err=errR2, multithread=TRUE)
#inspects the returned dadaFs object
dadaFs2[[1]]
#merges the forward and reverse reads and displays them
mergers2 <- mergePairs(dadaFs2, filtFs2, dadaRs2, filtRs2, verbose=TRUE)
#constructs an ASV table
seqtab2 <- makeSequenceTable(mergers2)
dim(seqtab2)
#inspects distribution of sequence lengths
table(nchar(getSequences(seqtab2)))
#
seqtab.nochim2 <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim2)
sum(seqtab.nochim2)/sum(seqtab2)
getN <- function(x) sum(getUniques(x))
track2 <- cbind(out, sapply(dadaFs2, getN), sapply(dadaRs2, getN), sapply(mergers2, getN), rowSums(seqtab.nochim2))
colnames(track2) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track2) <- sample.names
head(track2)
taxa2 <- assignTaxonomy(seqtab.nochim2, "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta", multithread=TRUE)
View(taxa2)
taxa2 <- assignTaxonomy(seqtab.nochim2, "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta", minBoot = 80, multithread=TRUE)
taxa2 <- assignTaxonomy(seqtab.nochim2, "/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta", multithread=TRUE)
View(taxa2)
taxa.print2 <- taxa2
rownames(taxa.print2) <- NULL
head(taxa.print2)
samples.out2 <- rownames(seqtab.nochim2)
treatment <- sapply(substring(samples.out2, 1, 1), `[`, 1)
geno <- sapply(substring(samples.out2, 2), `[`, 1)
samdf <- data.frame(treatment=treatment, geno=geno)
rownames(samdf) <- samples.out2
ps2 <- phyloseq(otu_table(seqtab.nochim2, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa2))
dna2 <- Biostrings::DNAStringSet(taxa_names(ps2))
names(dna2) <- taxa_names(ps2)
ps2 <- merge_phyloseq(ps2, dna2)
taxa_names(ps2) <- paste0("ASV", seq(ntaxa(ps2)))
ps.abund <- transform_sample_counts(ps2, function(OTU) OTU/sum(OTU))
graph_its<- ddply(subset(ps.abund,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ps_its <- psmelt(ps.abund)
graph_its<- ddply(subset(ps.its,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
graph_its<- ddply(subset(ps_its,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
View(ps_its)
View(samdf)
samdf <- read.csv("/Users/jackweldon/Desktop/Code/sampleIDS.csv", header = TRUE)
load("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/samdf.RData")
View(samdf)
ps2 <- phyloseq(otu_table(seqtab.nochim2, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa2))
rownames(samdf) <- samples.out2
View(samdf)
ps2 <- phyloseq(otu_table(seqtab.nochim2, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa2))
ps.abund <- transform_sample_counts(ps2, function(OTU) OTU/sum(OTU))
ps_its <- psmelt(ps.abund)
View(ps_its)
dna2 <- Biostrings::DNAStringSet(taxa_names(ps2))
names(dna2) <- taxa_names(ps2)
ps2 <- merge_phyloseq(ps2, dna2)
taxa_names(ps2) <- paste0("ASV", seq(ntaxa(ps2)))
ps.abund <- transform_sample_counts(ps2, function(OTU) OTU/sum(OTU))
ps_its <- psmelt(ps.abund)
View(ps_its)
graph_its<- ddply(subset(ps_its,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ggplot(graph_its_brown, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity") +
scale_fill_manual(values=c(pal(10),pal2(10), pal3(12)))+facet_wrap(~symstate)
graph_its_brown<- ddply(subset(ps_its,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ggplot(graph_its_brown, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity") +
scale_fill_manual(values=c(pal(10),pal2(10), pal3(12)))+facet_wrap(~symstate)
ggplot(graph_its_brown, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity") +
+facet_wrap(~symstate)
ggplot(graph_its_brown, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity")
View(graph_its_brown)
ps_its <- psmelt(ps2)
graph_its_brown<- ddply(subset(ps_its,symstate=='brown'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ggplot(graph_its_brown, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity")
View(graph_its_brown)
graph_its_white<- ddply(subset(ps_its,symstate=='white'), .(treatment), transform,
percent_abundance = (Abundance/sum(Abundance))* 100)
ggplot(graph_its_white, aes(x=treatment, y=percent_abundance, fill=Class))+geom_bar(stat="identity")
View(ps_its)
View(ps_otu_table)
View(ps_taxa_table)
library(phyloseq)
load("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/new_16S_analyses/bimeras.RData")
load("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/new_16S_analyses/taxa.RData")
load("/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/new_16S_analyses/taxab5.RData")
library(dada2)
library(dada2)
library(ShortRead)
library(Biostrings)
#path <- "/Users/jackweldon/Desktop/Code2/Astrangia_ITS_seqs"
path<-"/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Astrangia_ITS/input_files"
fnFs <- sort(list.files(path, pattern = "_R1_fordada.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_fordada.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), '[',1)
head(sample.names)
FWD <- "GTGAATTGCAGAACTCCGTG"  ## CHANGE ME to your forward primer sequence
REV <- "CCTCCGCTTACTTATATGCTT"  ## CHANGE ME...
allOrients <- function(primer) {
# Create all orientations of the input sequence
require(Biostrings)
dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
RevComp = reverseComplement(dna))
return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
primerHits <- function(primer, fn) {
# Counts number of reads in which the primer is found
nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]),
REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]),
REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
plotQualityProfile(fnFs.filtN[1:10])
plotQualityProfile(fnRs.filtN[1:10])
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(220,220), #leaves ~50bp overlap
maxN=0, #DADA does not allow Ns
maxEE=c(2,2), #allow 1 expected errors, where EE = sum(10^(-Q/10)); more conservative, model converges
truncQ=2,
trimLeft=c(0,20), #N nucleotides to remove from the start of each read
rm.phix=TRUE, #remove reads matching phiX genome
matchIDs=TRUE, #enforce matching between id-line sequence identifiers of F and R reads
compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
tail(out)
install.packages("Rmisc")
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
dadaFs[[1]]
dadaRs[[1]]
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
summary((mergers[[1]]))
View(derepFs)
View(dadaFs)
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE, minOverlap=5)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
summary((mergers[[1]]))
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE, minOverlap=5, maxMismatch = 3)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
summary((mergers[[1]]))
fnFs
fnRs
filtFs
filtRs
fnFs
fnRs
#path <- "/Users/jackweldon/Desktop/Code2/Astrangia_ITS_seqs"
path<-"/Users/hannyrivera/Documents/BU/DaviesLab/_Data/Oculina/16S-Jack/Astrangia_ITS/input_files"
fnFs <- sort(list.files(path, pattern = "_R1_fordada_astrangia_ITS.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_fordada_astrangia_ITS.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), '[',1)
head(sample.names)
FWD <- "GTGAATTGCAGAACTCCGTG"  ## CHANGE ME to your forward primer sequence
REV <- "CCTCCGCTTACTTATATGCTT"  ## CHANGE ME...
allOrients <- function(primer) {
# Create all orientations of the input sequence
require(Biostrings)
dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
RevComp = reverseComplement(dna))
return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
primerHits <- function(primer, fn) {
# Counts number of reads in which the primer is found
nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]),
REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]),
REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
plotQualityProfile(fnFs.filtN[1:10])
plotQualityProfile(fnRs.filtN[1:10])
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
truncLen=c(220,215), #leaves ~50bp overlap
maxN=0, #DADA does not allow Ns
maxEE=c(2,2),
truncQ=2,
rm.phix=TRUE,
matchIDs=TRUE,
compress=TRUE, multithread=TRUE)
head(out)
tail(out)
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
dadaFs[[1]]
dadaRs[[1]]
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
summary((mergers[[1]]))
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
plot(table(nchar(getSequences(seqtab))))
setwd("/Users/hannyrivera/Dou")
setwd("/Users/hannyrivera/Documents/Stella_Paper/Manuscript/Code/Data/")
source('~/Documents/Stella_Paper/Manuscript/Code/LT50_Analyses_Maternal-Paternal.R', echo=TRUE)
library(multcomp)
source('~/Documents/Stella_Paper/Manuscript/Code/LT50_Analyses_Maternal-Paternal.R', echo=TRUE)
source('~/Documents/Stella_Paper/Manuscript/Code/LT50_Analyses_Maternal-Paternal.R', echo=TRUE)
source('~/Documents/Stella_Paper/Manuscript/Code/LT50_Analyses_Maternal-Paternal.R', echo=TRUE)
